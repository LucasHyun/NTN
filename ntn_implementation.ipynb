{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasHyun/NTN/blob/main/ntn_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Hyperparameters and paths\n",
        "data_number = 0\n",
        "data_name = 'Wordnet' if data_number == 0 else 'Freebase'\n",
        "data_path = f'/content/drive/MyDrive/data/{data_name}'\n",
        "output_path = f'/content/drive/MyDrive/output/{data_name}/'\n",
        "num_iter = 500\n",
        "train_both = False\n",
        "batch_size = 20000\n",
        "corrupt_size = 10\n",
        "embedding_size = 100\n",
        "slice_size = 3\n",
        "regularization = 0.0001\n",
        "in_tensor_keep_normal = False\n",
        "save_per_iter = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "entities_string = '/entities.txt'\n",
        "relations_string = '/relations.txt'\n",
        "embeds_string = '/initEmbed.mat'\n",
        "training_string = '/train.txt'\n",
        "test_string = '/test.txt'\n",
        "dev_string = '/dev.txt'\n",
        "\n",
        "# Load functions\n",
        "def load_entities(data_path):\n",
        "    with open(data_path + entities_string, 'r') as f:\n",
        "        entities_list = f.read().strip().split('\\n')\n",
        "    return entities_list\n",
        "\n",
        "def load_relations(data_path):\n",
        "    with open(data_path + relations_string, 'r') as f:\n",
        "        relations_list = f.read().strip().split('\\n')\n",
        "    return relations_list\n",
        "\n",
        "def load_init_embeds(data_path):\n",
        "    embeds_path = data_path + embeds_string\n",
        "    return load_embeds(embeds_path)\n",
        "\n",
        "def load_embeds(file_path):\n",
        "    mat_contents = sio.loadmat(file_path)\n",
        "    words = mat_contents['words'].squeeze()\n",
        "    we = mat_contents['We']\n",
        "    tree = mat_contents['tree'].squeeze()\n",
        "    word_vecs = [we[:, i].tolist() for i in range(len(words))]\n",
        "    entity_words = [tree[i][0][0][0][0][0].item() for i in range(len(tree))]\n",
        "    return word_vecs, entity_words\n",
        "\n",
        "def load_training_data(data_path):\n",
        "    with open(data_path + training_string, 'r') as f:\n",
        "        training_data = [line.split('\\t') for line in f.read().strip().split('\\n')]\n",
        "    return np.array(training_data)\n",
        "\n",
        "def load_dev_data(data_path):\n",
        "    with open(data_path + dev_string, 'r') as f:\n",
        "        dev_data = [line.split('\\t') for line in f.read().strip().split('\\n')]\n",
        "    return np.array(dev_data)\n",
        "\n",
        "def load_test_data(data_path):\n",
        "    with open(data_path + test_string, 'r') as f:\n",
        "        test_data = [line.split('\\t') for line in f.read().strip().split('\\n')]\n",
        "    return np.array(test_data)\n",
        "\n",
        "# Helper functions\n",
        "def data_to_indexed(data, entities, relations):\n",
        "    entity_to_index = {entities[i]: i for i in range(len(entities))}\n",
        "    relation_to_index = {relations[i]: i for i in range(len(relations))}\n",
        "    indexed_data = [(entity_to_index[data[i][0]], relation_to_index[data[i][1]], entity_to_index[data[i][2]]) for i in range(len(data))]\n",
        "    return indexed_data\n",
        "\n",
        "def get_batch(batch_size, data, num_entities, corrupt_size):\n",
        "    random_indices = random.sample(range(len(data)), batch_size)\n",
        "    batch = [(data[i][0], data[i][1], data[i][2], random.randint(0, num_entities - 1))\n",
        "             for i in random_indices for _ in range(corrupt_size)]\n",
        "    return batch\n",
        "\n",
        "def split_batch(data_batch, num_relations):\n",
        "    batches = [[] for _ in range(num_relations)]\n",
        "    for e1, r, e2, e3 in data_batch:\n",
        "        batches[r].append((e1, e2, e3))\n",
        "    return batches\n",
        "\n",
        "def fill_feed_dict(batches, train_both, batch_placeholders, label_placeholders, corrupt_placeholder):\n",
        "    feed_dict = {corrupt_placeholder: [train_both and np.random.random() > 0.5]}\n",
        "    for i in range(len(batch_placeholders)):\n",
        "        feed_dict[batch_placeholders[i]] = batches[i]\n",
        "        feed_dict[label_placeholders[i]] = [[0.0] for _ in range(len(batches[i]))]\n",
        "    return feed_dict\n",
        "\n",
        "# Model functions\n",
        "def inference(batch_placeholders, corrupt_placeholder, init_word_embeds, entity_to_wordvec, num_entities, num_relations, slice_size, batch_size, is_eval, label_placeholders):\n",
        "    print(\"Building inference model...\")\n",
        "    d = embedding_size\n",
        "    k = slice_size\n",
        "    E = tf.Variable(init_word_embeds, dtype=tf.float32)  # d = embed size\n",
        "    W = [tf.Variable(tf.random.truncated_normal([d, d, k], stddev=0.1), name=f'W_{r}') for r in range(num_relations)]\n",
        "    V = [tf.Variable(tf.zeros([k, 2 * d]), name=f'V_{r}') for r in range(num_relations)]\n",
        "    b = [tf.Variable(tf.zeros([k, 1]), name=f'b_{r}') for r in range(num_relations)]\n",
        "    U = [tf.Variable(tf.ones([1, k]), name=f'U_{r}') for r in range(num_relations)]\n",
        "\n",
        "    ent2word = [tf.constant(entity_i.tolist(), dtype=tf.int32) - 1 for entity_i in entity_to_wordvec]\n",
        "    entEmbed = tf.stack([tf.reduce_mean(tf.gather(E, entword), axis=0) for entword in ent2word])\n",
        "\n",
        "    predictions = []\n",
        "    for r in range(num_relations):\n",
        "        e1, e2, e3 = tf.unstack(tf.cast(batch_placeholders[r], tf.int32), 3, axis=1)\n",
        "        e1v_pos = tf.transpose(tf.gather(entEmbed, e1), perm=[1, 0])\n",
        "        e2v_pos = tf.transpose(tf.gather(entEmbed, e2), perm=[1, 0])\n",
        "        e1v_neg = e1v_pos\n",
        "        e2v_neg = tf.transpose(tf.gather(entEmbed, e3), perm=[1, 0])\n",
        "        num_rel_r = tf.shape(e1v_pos)[1]\n",
        "\n",
        "        preactivation_pos = []\n",
        "        preactivation_neg = []\n",
        "        for slice in range(k):\n",
        "            preactivation_pos.append(tf.reduce_sum(e1v_pos * tf.matmul(W[r][:, :, slice], e2v_pos), axis=0))\n",
        "            preactivation_neg.append(tf.reduce_sum(e1v_neg * tf.matmul(W[r][:, :, slice], e2v_neg), axis=0))\n",
        "        preactivation_pos = tf.stack(preactivation_pos, axis=1)\n",
        "        preactivation_neg = tf.stack(preactivation_neg, axis=1)\n",
        "\n",
        "        temp2_pos = tf.matmul(V[r], tf.concat([e1v_pos, e2v_pos], axis=0))\n",
        "        temp2_neg = tf.matmul(V[r], tf.concat([e1v_neg, e2v_neg], axis=0))\n",
        "\n",
        "        preactivation_pos = preactivation_pos + temp2_pos + b[r]\n",
        "        preactivation_neg = preactivation_neg + temp2_neg + b[r]\n",
        "\n",
        "        activation_pos = tf.nn.tanh(preactivation_pos)\n",
        "        activation_neg = tf.nn.tanh(preactivation_neg)\n",
        "\n",
        "        score_pos = tf.squeeze(tf.matmul(U[r], activation_pos))\n",
        "        score_neg = tf.squeeze(tf.matmul(U[r], activation_neg))\n",
        "\n",
        "        if not is_eval:\n",
        "            predictions.append(tf.stack([score_pos, score_neg], axis=1))\n",
        "        else:\n",
        "            predictions.append(tf.stack([score_pos, tf.squeeze(label_placeholders[r])], axis=1))\n",
        "\n",
        "    predictions = tf.concat(predictions, axis=0)\n",
        "    return predictions\n",
        "\n",
        "def loss(predictions, regularization):\n",
        "    print(\"Building loss function...\")\n",
        "    margin = 1.0\n",
        "    labels = tf.constant([1.0, -1.0])\n",
        "    temp1 = tf.maximum(0.0, margin - (predictions[:, 0] - predictions[:, 1]))\n",
        "    temp1 = tf.reduce_sum(temp1)\n",
        "    temp2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
        "    loss_val = temp1 + regularization * temp2\n",
        "    return loss_val\n",
        "\n",
        "def training(loss, learning_rate):\n",
        "    print(\"Building training operation...\")\n",
        "    optimizer = tf.compat.v1.train.AdagradOptimizer(learning_rate)\n",
        "    train_op = optimizer.minimize(loss)\n",
        "    return train_op\n",
        "\n",
        "def evaluation(predictions):\n",
        "    print(\"Building evaluation operation...\")\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(predictions, 1), tf.argmax(tf.constant([1.0, 0.0]), 1)), tf.float32))\n",
        "\n",
        "# Training function\n",
        "def run_training():\n",
        "    print(\"Starting training process...\")\n",
        "    raw_training_data = load_training_data(data_path)\n",
        "    raw_dev_data = load_dev_data(data_path)\n",
        "    raw_test_data = load_test_data(data_path)\n",
        "\n",
        "    entities_list = load_entities(data_path)\n",
        "    relations_list = load_relations(data_path)\n",
        "    init_word_embeds, entity_to_wordvec = load_init_embeds(data_path)\n",
        "\n",
        "    training_data = data_to_indexed(raw_training_data, entities_list, relations_list)\n",
        "    dev_data = data_to_indexed(raw_dev_data, entities_list, relations_list)\n",
        "    test_data = data_to_indexed(raw_test_data, entities_list, relations_list)\n",
        "\n",
        "    num_entities = len(entities_list)\n",
        "    num_relations = len(relations_list)\n",
        "\n",
        "    init_word_embeds = np.array(init_word_embeds, dtype=np.float32)\n",
        "    entity_to_wordvec = [np.array(words, dtype=np.int32) for words in entity_to_wordvec]\n",
        "\n",
        "    batch_placeholders = [tf.compat.v1.placeholder(tf.int32, shape=(None, 3)) for _ in range(num_relations)]\n",
        "    label_placeholders = [tf.compat.v1.placeholder(tf.float32, shape=(None, 1)) for _ in range(num_relations)]\n",
        "    corrupt_placeholder = tf.compat.v1.placeholder(tf.bool, shape=())\n",
        "\n",
        "    is_eval = tf.compat.v1.placeholder(tf.bool)\n",
        "\n",
        "    predictions = inference(batch_placeholders, corrupt_placeholder, init_word_embeds, entity_to_wordvec,\n",
        "                            num_entities, num_relations, slice_size, batch_size, is_eval, label_placeholders)\n",
        "\n",
        "    loss_op = loss(predictions, regularization)\n",
        "    train_op = training(loss_op, learning_rate)\n",
        "    eval_op = evaluation(predictions)\n",
        "\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        for step in range(num_iter):\n",
        "            batch = get_batch(batch_size, training_data, num_entities, corrupt_size)\n",
        "            batches = split_batch(batch, num_relations)\n",
        "            feed_dict = fill_feed_dict(batches, train_both, batch_placeholders, label_placeholders, corrupt_placeholder)\n",
        "\n",
        "            _, loss_value = sess.run([train_op, loss_op], feed_dict=feed_dict)\n",
        "\n",
        "            if step % save_per_iter == 0:\n",
        "                dev_batch = get_batch(batch_size, dev_data, num_entities, corrupt_size)\n",
        "                dev_batches = split_batch(dev_batch, num_relations)\n",
        "                dev_feed_dict = fill_feed_dict(dev_batches, train_both, batch_placeholders, label_placeholders, corrupt_placeholder)\n",
        "                dev_feed_dict[is_eval] = True\n",
        "                accuracy = sess.run(eval_op, feed_dict=dev_feed_dict)\n",
        "                print(f'Step {step}: loss = {loss_value:.2f}, dev accuracy = {accuracy:.2f}')\n",
        "\n",
        "        test_batch = get_batch(batch_size, test_data, num_entities, corrupt_size)\n",
        "        test_batches = split_batch(test_batch, num_relations)\n",
        "        test_feed_dict = fill_feed_dict(test_batches, train_both, batch_placeholders, label_placeholders, corrupt_placeholder)\n",
        "        test_feed_dict[is_eval] = True\n",
        "        accuracy = sess.run(eval_op, feed_dict=test_feed_dict)\n",
        "        print(f'Test accuracy = {accuracy:.2f}')\n",
        "\n",
        "run_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "YW14sIUVCJyL",
        "outputId": "7ba0d1b5-5fff-458d-b5a8-106d5fed69a6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training process...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "tf.placeholder() is not compatible with eager execution.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-bf2ccad1fa40>\u001b[0m in \u001b[0;36m<cell line: 224>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test accuracy = {accuracy:.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-bf2ccad1fa40>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mentity_to_wordvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentity_to_wordvec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mbatch_placeholders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_relations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mlabel_placeholders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_relations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mcorrupt_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-bf2ccad1fa40>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mentity_to_wordvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentity_to_wordvec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mbatch_placeholders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_relations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mlabel_placeholders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_relations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mcorrupt_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   3269\u001b[0m   \"\"\"\n\u001b[1;32m   3270\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3271\u001b[0;31m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[1;32m   3272\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   3273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "# Placeholder를 사용하기 위해 그래프 모드로 변경\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# Load data\n",
        "data_path = './drive/MyDrive/data/Wordnet'\n",
        "entities_string = '/entities.txt'\n",
        "relations_string = '/relations.txt'\n",
        "embeds_string = '/initEmbed.mat'\n",
        "training_string = '/train.txt'\n",
        "test_string = '/test.txt'\n",
        "dev_string = '/dev.txt'\n",
        "\n",
        "def load_entities(data_path):\n",
        "    entities_file = open(data_path + entities_string)\n",
        "    entities_list = entities_file.read().strip().split('\\n')\n",
        "    entities_file.close()\n",
        "    return entities_list\n",
        "\n",
        "def load_relations(data_path):\n",
        "    relations_file = open(data_path + relations_string)\n",
        "    relations_list = relations_file.read().strip().split('\\n')\n",
        "    relations_file.close()\n",
        "    return relations_list\n",
        "\n",
        "def load_init_embeds(data_path):\n",
        "    embeds_path = data_path + embeds_string\n",
        "    return load_embeds(embeds_path)\n",
        "\n",
        "def load_embeds(file_path):\n",
        "    mat_contents = sio.loadmat(file_path)\n",
        "    words = mat_contents['words'].squeeze()\n",
        "    we = mat_contents['We']\n",
        "    tree = mat_contents['tree'].squeeze()\n",
        "    word_vecs = [we[:, i].tolist() for i in range(len(words))]\n",
        "    entity_words = [tree[i][0][0][0][0][0].item() for i in range(len(tree))]\n",
        "    return word_vecs, entity_words\n",
        "\n",
        "def load_data(data_path, file_string):\n",
        "    file = open(data_path + file_string)\n",
        "    data = [line.split('\\t') for line in file.read().strip().split('\\n')]\n",
        "    return np.array(data)\n",
        "\n",
        "def data_to_indexed(data, entities, relations):\n",
        "    entity_to_index = {entities[i]: i for i in range(len(entities))}\n",
        "    relation_to_index = {relations[i]: i for i in range(len(relations))}\n",
        "    indexed_data = [(entity_to_index[data[i][0]], relation_to_index[data[i][1]], entity_to_index[data[i][2]]) for i in range(len(data))]\n",
        "    return indexed_data\n",
        "\n",
        "def get_batch(batch_size, data, num_entities, corrupt_size):\n",
        "    random_indices = random.sample(range(len(data)), batch_size)\n",
        "    batch = [(data[i][0], data[i][1], data[i][2], random.randint(0, num_entities - 1)) for i in random_indices for _ in range(corrupt_size)]\n",
        "    return batch\n",
        "\n",
        "def split_batch(data_batch, num_relations):\n",
        "    batches = [[] for _ in range(num_relations)]\n",
        "    for e1, r, e2, e3 in data_batch:\n",
        "        batches[r].append((e1, e2, e3))\n",
        "    return batches\n",
        "\n",
        "def fill_feed_dict(batches, train_both, batch_placeholders, label_placeholders, corrupt_placeholder):\n",
        "    feed_dict = {corrupt_placeholder: [train_both and np.random.random() > 0.5]}\n",
        "    for i in range(len(batch_placeholders)):\n",
        "        feed_dict[batch_placeholders[i]] = batches[i]\n",
        "        feed_dict[label_placeholders[i]] = [[0.0] for _ in range(len(batches[i]))]\n",
        "    return feed_dict\n",
        "\n",
        "def run_training():\n",
        "    print(\"Begin!\")\n",
        "    print(\"Load training data...\")\n",
        "    raw_training_data = load_data(data_path, training_string)\n",
        "    print(\"Load entities and relations...\")\n",
        "    entities_list = load_entities(data_path)\n",
        "    relations_list = load_relations(data_path)\n",
        "    indexed_training_data = data_to_indexed(raw_training_data, entities_list, relations_list)\n",
        "    print(\"Load embeddings...\")\n",
        "    init_word_embeds, entity_to_wordvec = load_init_embeds(data_path)\n",
        "\n",
        "    num_entities = len(entities_list)\n",
        "    num_relations = len(relations_list)\n",
        "    num_iters = 500\n",
        "    batch_size = 20000\n",
        "    corrupt_size = 10\n",
        "    slice_size = 3\n",
        "    regularization = 0.0001\n",
        "    learning_rate = 0.01\n",
        "    save_per_iter = 10\n",
        "    train_both = False\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "        print(f\"Starting to build graph {datetime.datetime.now()}\")\n",
        "        batch_placeholders = [tf.compat.v1.placeholder(tf.int32, shape=(None, 3), name=f'batch_{i}') for i in range(num_relations)]\n",
        "        label_placeholders = [tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=f'label_{i}') for i in range(num_relations)]\n",
        "        corrupt_placeholder = tf.compat.v1.placeholder(tf.bool, shape=())\n",
        "\n",
        "        # Build the computational graph\n",
        "        predictions = inference(batch_placeholders, corrupt_placeholder, init_word_embeds, entity_to_wordvec,\n",
        "                                num_entities, num_relations, slice_size, batch_size, False, label_placeholders)\n",
        "        loss_op = loss(predictions, regularization)\n",
        "        train_op = training(loss_op, learning_rate, tf.compat.v1.trainable_variables())\n",
        "\n",
        "        # Create a session for running Ops on the Graph.\n",
        "        sess = tf.compat.v1.Session()\n",
        "\n",
        "        init = tf.compat.v1.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables())\n",
        "\n",
        "        for i in range(1, num_iters + 1):\n",
        "            print(f\"Starting iter {i} {datetime.datetime.now()}\")\n",
        "            data_batch = get_batch(batch_size, indexed_training_data, num_entities, corrupt_size)\n",
        "            relation_batches = split_batch(data_batch, num_relations)\n",
        "\n",
        "            if i % save_per_iter == 0:\n",
        "                saver.save(sess, f\"{output_path}/{data_name}{i}.sess\")\n",
        "\n",
        "            feed_dict = fill_feed_dict(relation_batches, train_both, batch_placeholders, label_placeholders, corrupt_placeholder)\n",
        "            _, loss_value = sess.run([train_op, loss_op], feed_dict=feed_dict)\n",
        "\n",
        "def inference(batch_placeholders, corrupt_placeholder, init_word_embeds, entity_to_wordvec, num_entities, num_relations, slice_size, batch_size, is_eval, label_placeholders):\n",
        "    # Build the inference graph\n",
        "    d = 100\n",
        "    k = slice_size\n",
        "    ten_k = tf.constant([k])\n",
        "    num_words = len(init_word_embeds)\n",
        "    E = tf.Variable(init_word_embeds)\n",
        "    W = [tf.Variable(tf.random.truncated_normal([d, d, k])) for r in range(num_relations)]\n",
        "    V = [tf.Variable(tf.zeros([k, 2 * d])) for r in range(num_relations)]\n",
        "    b = [tf.Variable(tf.zeros([k, 1])) for r in range(num_relations)]\n",
        "    U = [tf.Variable(tf.ones([1, k]), name=f'U_{r}') for r in range(num_relations)]\n",
        "\n",
        "    ent2word = [tf.constant([entity_i], dtype=tf.int32) - 1 for entity_i in entity_to_wordvec]\n",
        "    entEmbed = tf.stack([tf.reduce_mean(tf.gather(E, entword), axis=0) for entword in ent2word])\n",
        "\n",
        "    predictions = []\n",
        "    for r in range(num_relations):\n",
        "        e1, e2, e3 = [tf.squeeze(t) for t in tf.unstack(tf.cast(batch_placeholders[r], tf.int32), 3, axis=1)]\n",
        "        e1v = tf.transpose(tf.squeeze(tf.gather(entEmbed, e1), axis=0))\n",
        "        e2v = tf.transpose(tf.squeeze(tf.gather(entEmbed, e2), axis=0))\n",
        "        e3v = tf.transpose(tf.squeeze(tf.gather(entEmbed, e3), axis=1))\n",
        "        e1v_pos = e1v\n",
        "        e2v_pos = e2v\n",
        "        e1v_neg = e1v\n",
        "        e2v_neg = e3v\n",
        "        num_rel_r = tf.expand_dims(tf.shape(e1v_pos)[0], 0)\n",
        "        preactivation_pos = []\n",
        "        preactivation_neg = []\n",
        "\n",
        "        for slice in range(k):\n",
        "            preactivation_pos.append(tf.reduce_sum(e1v_pos * tf.matmul(W[r][:, :, slice], e2v_pos), axis=1))\n",
        "            preactivation_neg.append(tf.reduce_sum(e1v_neg * tf.matmul(W[r][:, :, slice], e2v_neg), axis=1))\n",
        "\n",
        "        preactivation_pos = tf.stack(preactivation_pos, axis=1)\n",
        "        preactivation_neg = tf.stack(preactivation_neg, axis=1)\n",
        "\n",
        "        temp2_pos = tf.matmul(V[r], tf.concat([e1v_pos, e2v_pos], axis=1))\n",
        "        temp2_neg = tf.matmul(V[r], tf.concat([e1v_neg, e2v_neg], axis=1))\n",
        "\n",
        "        preactivation_pos = preactivation_pos + temp2_pos + b[r]\n",
        "        preactivation_neg = preactivation_neg + temp2_neg + b[r]\n",
        "\n",
        "        activation_pos = tf.math.tanh(preactivation_pos)\n",
        "        activation_neg = tf.math.tanh(preactivation_neg)\n",
        "\n",
        "        score_pos = tf.reshape(tf.matmul(U[r], activation_pos), num_rel_r)\n",
        "        score_neg = tf.reshape(tf.matmul(U[r], activation_neg), num_rel_r)\n",
        "\n",
        "        if not is_eval:\n",
        "            predictions.append(tf.stack([score_pos, score_neg], axis=1))\n",
        "        else:\n",
        "            predictions.append(tf.stack([score_pos, tf.reshape(label_placeholders[r], num_rel_r)], axis=1))\n",
        "\n",
        "    predictions = tf.concat(predictions, axis=1)\n",
        "    return predictions\n",
        "\n",
        "def loss(predictions, regularization):\n",
        "    temp1 = tf.maximum(tf.subtract(predictions[:, 1], predictions[:, 0]) + 1, 0)\n",
        "    temp1 = tf.reduce_sum(temp1)\n",
        "    temp2 = tf.sqrt(sum([tf.reduce_sum(tf.square(var)) for var in tf.compat.v1.trainable_variables()]))\n",
        "    temp = temp1 + (regularization * temp2)\n",
        "    return temp\n",
        "\n",
        "def training(loss, learning_rate, var_list):\n",
        "    optimizer = tf.optimizers.Adagrad(learning_rate)\n",
        "    return optimizer.minimize(loss, var_list=var_list)\n",
        "\n",
        "run_training()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "CQ2WC9wPEUEV",
        "outputId": "0a5e5bd6-9e23-4adf-a7cf-7760dbb9901d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin!\n",
            "Load training data...\n",
            "Load entities and relations...\n",
            "Load embeddings...\n",
            "Starting to build graph 2024-05-18 01:01:12.702258\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`tape` is required when a `Tensor` loss is passed. Received: loss=Tensor(\"add_90:0\", shape=(), dtype=float32), tape=None.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-c3aaede604f2>\u001b[0m in \u001b[0;36m<cell line: 192>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-77-c3aaede604f2>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m                                 num_entities, num_relations, slice_size, batch_size, False, label_placeholders)\n\u001b[1;32m    104\u001b[0m         \u001b[0mloss_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Create a session for running Ops on the Graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-c3aaede604f2>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(loss, learning_rate, var_list)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    541\u001b[0m           \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \"\"\"\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0;34m\"`tape` is required when a `Tensor` loss is passed. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;34mf\"Received: loss={loss}, tape={tape}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `tape` is required when a `Tensor` loss is passed. Received: loss=Tensor(\"add_90:0\", shape=(), dtype=float32), tape=None."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFB24LSsEEq2YjH3kAa7SD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}